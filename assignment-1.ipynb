{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415baec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: textblob in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (3.8.11)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/NLP/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas textblob nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279e42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/win/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/win/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4c68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # replace new line and tab with space\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def process_textblob(text):\n",
    "    # Create blob from raw text so there still . , for tokenizing sentence\n",
    "    blob_raw = TextBlob(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [str(s) for s in blob_raw.sentences]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "    \n",
    "    # Create blob form cleaned text to tokenize word\n",
    "    cleaned_text = clean(text)\n",
    "    # Create blob from cleaned text\n",
    "    blob_cleaned = TextBlob(cleaned_text)\n",
    "\n",
    "    # Filter stop word\n",
    "    filtered_words = [w for w in blob_cleaned.words if w not in stop_words]\n",
    "    \n",
    "    # Top Word\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "    \n",
    "    return cleaned_text, cleaned_sentences, filtered_words, top_words\n",
    "\n",
    "def process_nltk(text):\n",
    "    # Tokenize sentence\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Clean sentence\n",
    "    cleaned_sentence = [clean(s) for s in sentences]\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = clean(text)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = nltk.word_tokenize(cleaned_text)\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, cleaned_sentence, filtered_words, top_words\n",
    "\n",
    "def process_spacy(text):\n",
    "    # run spacy on raw text to keep . , for sentences tokenization\n",
    "    spacy_raw = spacy_nlp(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [s.text.strip() for s in spacy_raw.sents]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "\n",
    "    cleaned_text = clean(text)\n",
    "    spacy_cleand = spacy_nlp(cleaned_text)\n",
    "    # Tokenize words\n",
    "    filtered_words = [w.text.strip() for w in spacy_cleand if w not in stop_words]\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, sentences, filtered_words, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5889c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reports(strategy_name, clean_text, sentences, words, top_words, elapsed):\n",
    "    # Create output directory\n",
    "    output_dir = \"output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save cleaned text\n",
    "    with open(f\"{output_dir}/cleaned_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    # Save tokenize sentence and word with their count\n",
    "    with open(f\"{output_dir}/words_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"----Tokenized Sentences ({len(sentences)})----\\n\")\n",
    "        f.write(\"\\n\".join(f\"{s}\" for s in  sentences))\n",
    "        f.write(f\"----\\n\\nTokenized Words ({len(words)})----\\n\")\n",
    "        f.write(\"\\n\".join(words))\n",
    "\n",
    "    # convert tuple to dataframe and save to textfile\n",
    "    df_top = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])\n",
    "    with open(f\"{output_dir}/top10words_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df_top.to_string(index=False))\n",
    "\n",
    "    time_file = f\"{output_dir}/time_compares.txt\"\n",
    "    \n",
    "    # create data frame for the current run\n",
    "    new_row = {\"Strategy\": strategy_name, \"Time(s)\": round(elapsed, 6)}\n",
    "    df_new = pd.DataFrame([new_row])\n",
    "    \n",
    "    # check if time compare file already exist or not\n",
    "    if os.path.exists(time_file):\n",
    "        # append new row to the old one if the file exist\n",
    "        df_existing = pd.read_csv(time_file)\n",
    "        df_final = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        # if not then it just the row\n",
    "        df_final = df_new\n",
    "    \n",
    "    # save the file with out row number\n",
    "    df_final.to_csv(time_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy pattern at home\n",
    "strategies = [\n",
    "    (process_textblob, \"TextBlob\"),\n",
    "    (process_nltk, \"NLTK\"),\n",
    "    (process_spacy, \"spaCy\")\n",
    "]\n",
    "\n",
    "# Remove previous timecompared.txt to prevent the result of new run to be append to the old file\n",
    "time_file_path = \"output/time_compares.txt\"\n",
    "if os.path.exists(time_file_path):\n",
    "    os.remove(time_file_path)\n",
    "\n",
    "input_file = \"resource/alice29.txt\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# loop through each strategy \n",
    "for strategy_func, name in strategies:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # run the strategy\n",
    "    cleaned_text, sentences, final_words_str, top = strategy_func(raw_text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # record run time\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    # create report for each strategies\n",
    "    save_reports(name, cleaned_text, sentences, final_words_str, top, elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
