{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415baec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas textblob nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "279e42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/win/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/win/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d4c68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # replace new line and tab with space\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def process_textblob(text):\n",
    "    # Create blob from raw text so there still . , for tokenizing sentence\n",
    "    blob_raw = TextBlob(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [str(s) for s in blob_raw.sentences]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "    \n",
    "    # Create blob form cleaned text to tokenize word\n",
    "    cleaned_text = clean(text)\n",
    "    # Create blob from cleaned text\n",
    "    blob_cleaned = TextBlob(cleaned_text)\n",
    "\n",
    "    # Filter stop word\n",
    "    filtered_words = [w for w in blob_cleaned.words if w not in stop_words]\n",
    "    \n",
    "    # Top Word\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "    \n",
    "    return cleaned_text, cleaned_sentences, filtered_words, top_words\n",
    "\n",
    "def process_nltk(text):\n",
    "    # Tokenize sentence\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Clean sentence\n",
    "    cleaned_sentence = [clean(s) for s in sentences]\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = clean(text)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = nltk.word_tokenize(cleaned_text)\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, cleaned_sentence, filtered_words, top_words\n",
    "\n",
    "def process_spacy(text):\n",
    "    # run spacy on raw text to keep . , for sentences tokenization\n",
    "    spacy_raw = spacy_nlp(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [s.text.strip() for s in spacy_raw.sents]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "\n",
    "    cleaned_text = clean(text)\n",
    "    spacy_cleand = spacy_nlp(cleaned_text)\n",
    "    # Tokenize words\n",
    "    filtered_words = [w.text.strip() for w in spacy_cleand if w not in stop_words]\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, sentences, filtered_words, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5889c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reports(strategy_name, clean_text, sentences, words, top_words, elapsed):\n",
    "    # Create output directory\n",
    "    output_dir = \"output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save cleaned text\n",
    "    with open(f\"{output_dir}/cleaned_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    # Save tokenize sentence and word with their count\n",
    "    with open(f\"{output_dir}/words_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"----Tokenized Sentences ({len(sentences)})----\\n\")\n",
    "        f.write(\"\\n\".join(f\"{s}\" for s in  sentences))\n",
    "        f.write(f\"----\\n\\nTokenized Words ({len(words)})----\\n\")\n",
    "        f.write(\"\\n\".join(words))\n",
    "\n",
    "    # convert tuple to dataframe and save to textfile\n",
    "    df_top = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])\n",
    "    with open(f\"{output_dir}/top10words_{strategy_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df_top.to_string(index=False))\n",
    "\n",
    "    time_file = f\"{output_dir}/time_compares.txt\"\n",
    "    \n",
    "    # create data frame for the current run\n",
    "    new_row = {\"Strategy\": strategy_name, \"Time(s)\": round(elapsed, 6)}\n",
    "    df_new = pd.DataFrame([new_row])\n",
    "    \n",
    "    # check if time compare file already exist or not\n",
    "    if os.path.exists(time_file):\n",
    "        # append new row to the old one if the file exist\n",
    "        df_existing = pd.read_csv(time_file)\n",
    "        df_final = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        # if not then it just the row\n",
    "        df_final = df_new\n",
    "    \n",
    "    # save the file with out row number\n",
    "    df_final.to_csv(time_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f54408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy pattern at home\n",
    "strategies = [\n",
    "    (process_textblob, \"TextBlob\"),\n",
    "    (process_nltk, \"NLTK\"),\n",
    "    (process_spacy, \"spaCy\")\n",
    "]\n",
    "\n",
    "# Remove previous timecompared.txt to prevent the result of new run to be append to the old file\n",
    "time_file_path = \"output/time_compares.txt\"\n",
    "if os.path.exists(time_file_path):\n",
    "    os.remove(time_file_path)\n",
    "\n",
    "input_file = \"resource/alice29.txt\"\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# loop through each strategy \n",
    "for strategy_func, name in strategies:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # run the strategy\n",
    "    cleaned_text, sentences, final_words_str, top = strategy_func(raw_text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # record run time\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    # create report for each strategies\n",
    "    save_reports(name, cleaned_text, sentences, final_words_str, top, elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
